# TraceFL: Interpretability-Driven Debugging in Federated Learning via Neuron Provenance

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/warisgill/TraceFL-Artifact/blob/main/artifact.ipynb)
![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)

**Paper:** [arXiv Preprint](https://arxiv.org/abs/2312.13632)  
**Artifact Archive:** [Zenodo Permanent Record](https://zenodo.org/records/12345678)

**Authors:** [Waris Gill](https://people.cs.vt.edu/waris/), [Ali Anwar](https://chalianwar.github.io/), [Muhammad Ali Gulzar](https://people.cs.vt.edu/~gulzar/)

## 1. Purpose
**TraceFL** is the first interpretabilty techniques that enables interpretability in Federated Learning (FL) by identifying clients responsible for specific global model predictions.  By making such provenance information explicit, developers can **exclude** problematic clients, **reward** high-quality clients, or **debug** misclassifications more systematically.

<!-- Add a figure here -->

**This artifact provides:**

-  Complete implementation of the TraceFL framework
-  Pre-configured experiments replicating all paper results
-  Cross-domain support for image/text classification models (e.g., GPT )
- **One-click reproducibility on Google Colab.**
<!-- add the colab icon above -->


**Badges Claimed**:
- **Artifacts Available**: All our code and documentation are publicly and permanently archived ([Zenodo DOI](https://doi.org/xx.xxxx/zenodo.xxxxxx)).
- **Artifacts Functional**: We include step-by-step installation scripts, test commands, and evidence of correct behavior in a minimal environment.
- **Artifacts Reusable**: We offer detailed documentation, consistent structure, modular design, a permissive license, and instructions for extending the framework to new models/datasets.



## 2. Provenance

- **Paper Preprint:** [ICSE 2025 Camera-Ready](https://arxiv.org/pdf/2312.13632)
  
- **Archived Artifact**: The exact version of this repository (including code, configurations, and instructions) is archived at **[Zenodo](https://doi.org/xx.xxxx/zenodo.xxxxxx)**.  
- **GitHub Repository** (development version): [GitHub - warisgill/TraceFL-Artifact](https://github.com/warisgill/TraceFL-Artifact) (non-archival).  

- **License:** [MIT License](LICENSE)


## 3. Data

TraceFL is a **domain-agnostic** framework carefully designed to handle various data modalities (vision, text, and medical). We demonstrate its flexibility by evaluating multiple **image**, **medical imaging**, and **text** datasets, as well as different **neural architectures** ranging from classic **CNNs** to **transformers**.

### Datasets
   - **Image Classification**: *CIFAR-10, MNIST* (public benchmarks with 10 classes each).  
   - **Medical Imaging**: *Colon PathMNIST* and *Abdominal OrganAMNIST* from [MedMNIST](https://medmnist.com/). These datasets are curated, de-identified, and suitable for research in FL-based medical imaging.  
   - **Text Classification**: *DBpedia, Yahoo Answers* (both standard benchmarks in natural language processing).

   All datasets are publicly available. We follow [FlowerDatasets](https://flower.ai/docs/datasets/index.html) guidelines to download, partition, and integrate these datasets with minimal configuration overhead. 

### Models  
   - **Image Classification Models**: 
     - *ResNet* (e.g., `resnet18`, `resnet50`)  
     - *DenseNet* (e.g., `densenet121`)  
   - **Medical Imaging**: Same CNN-based architectures (ResNet, DenseNet) easily adapted for grayscale inputs or domain-specific classification tasks.  
   - **Text Classification Models**:
     - *GPT*  
     - *BERT*  
   
   TraceFL uses a consistent interpretability mechanism at the **neuron** level, which naturally extends to different layers and architectures. This ensures minimal or no code changes are needed to debug new classification models—so long as they output logits for classification.


## 4. Setup


**System Requirements**:
- **Orginal Paper Hardware Setup**: To resemble real-world FL and do large scale simulations, we deploy our experiments in [Flower FL framework](https://flower.ai/), running on an enterprise-level cluster of six NVIDIA DGX A100 nodes. Each node is equipped with 2048 GB of memory, at least 128 cores, and an A100 GPU with 80 GB of memory.

- **Artifact Hardware Setup**: We change the default configuration in `tracefl/conf/base.yaml` to run representative experiments on Google Colab even with only 2 cpu cores, 12 GB of System RAM and 15 GB of GPU RAM. 


We provide **three** approaches to setting up the environment:

## 4.1 Quick Colab Setup 

To quickly validate the artifact, click the "Open in Colab" badge above. This will open a Google Colab notebook with all dependencies pre-installed. You can run the provided demo script to verify the installation and generate a sample provenance report.


### 4.2 Local/Conda Setup

1. **Create Conda environment** (Python 3.10):
   ```bash
   conda create --name tracefl python=3.10 -y
   conda activate tracefl
   ```
2. **Install Poetry**:
   ```bash
   pip install poetry
   ```
3. **Clone and install dependencies**:
   ```bash
   git clone https://github.com/warisgill/TraceFL-Artifact.git
   cd TraceFL-Artifact
   poetry install
   ```
   **Expected Output:**  
   `✅ TraceFL installed successfully! Ready for federated interpretability!`





### 4.3 Quick Docker Setup (Recommended)

We offer a Docker image for consistent, frictionless installation:
```bash
# 1. Clone this repository
git clone https://github.com/warisgill/TraceFL-Artifact.git
cd TraceFL-Artifact

# 2. Build the Docker image
docker build -t tracefl:latest .

# 3. Run the container (maps a local port if needed)
docker run -it --gpus all --name tracefl_container tracefl:latest
```
Inside the container, you can run all commands exactly as described below.



## 5. Usage

### 5.1 Quick “Smoke Test” (Minimal Example)

We provide a tiny test script to confirm correct installation in under 5 minutes:
```bash
# Inside your Docker container or after activating the tracefl conda env:
python -m tracefl.main --model=resnet18 --dataset=mnist \
  --num_clients=2 --clients_per_round=2 --num_rounds=1 \
  --client.epochs=1 --batch_size=8
```
**Expected Outcome**:  
- The script trains a small FL setup with 2 clients on MNIST for 1 round.  
- If successful, the console logs will indicate completion with a message like:  
  ```
  [INFO] Training round 1/1 completed. 
  [INFO] TraceFL debug pass completed successfully!
  ```
- Total runtime is about ~2-3 minutes on CPU, <1 minute on a GPU-enabled machine.

### 5.2 Reproducing Main Paper Experiments

1. **Default Configuration** (example run):
   ```bash
   # By default, reads from tracefl/conf/base.yaml
   python -m tracefl.main
   ```
   This trains a DenseNet model on PathMNIST with 10 clients, demonstrating how to replicate the *core approach* from the paper.

2. **Specific Figures/Tables**:
   - **Figure 2, Table 3, Figure 5**: 
     ```bash
     chmod +x figure.sh
     ./figure.sh
     ```
     This script runs multiple dataset/model configurations and logs results to `logs/`.
   - **Table 2, Figure 4**:
     ```bash
     chmod +x figure4.sh
     ./figure4.sh
     ```
   - **Table 1, Figure 6**:
     ```bash
     chmod +x table1.sh
     ./table1.sh
     ```

3. **Google Colab**:  
   - Open [artifact.ipynb](https://colab.research.google.com/github/warisgill/TraceFL-Artifact/blob/main/artifact.ipynb) directly in Colab for a one-click environment.

### 5.3 Extending/Repurposing TraceFL

- **Switching Models**: Use any HuggingFace model name (e.g., `bert-base-cased`) or a known vision model (`resnet18`, `densenet121`) in the command line or `base.yaml`.  
- **Switching Datasets**: Provide any classification dataset recognized by [FlowerDatasets](https://flower.ai/docs/datasets/index.html), or adapt the YAML config to your custom dataset.  
- **Customizing Hyperparameters**: Edit `tracefl/conf/base.yaml` or pass flags (e.g., `--num_rounds`, `--dirichlet_alpha`) directly to `python -m tracefl.main`.


### 5.4. Evidence of Correctness

- **Comparison to FedDebug**: We include scripts in `table1.sh` for Table 1, showcasing how TraceFL outperforms FedDebug in localizing responsible clients.  
- **Accuracy & Scalability**: Scripts in `figure.sh` and `figure4.sh` replicate the main results (over 20,000+ client models in the original paper).  
- **Logging and Outputs**: All scripts produce logs in `logs/`. Compare them to sample logs in `logs/sample_output_reference/` for verification.


## 6 License
This artifact is released under [MIT License](LICENSE), enabling:
- Commercial use
- Modification
- Distribution
- Private use


## 7. How This Artifact Meets ICSE Criteria

1. **Available**  
   - Permanently hosted on Zenodo ([DOI](https://doi.org/xx.xxxx/zenodo.xxxxxx)) and supplemented on GitHub.  

2. **Functional**  
   - Documented installation procedures.  
   - Includes a quick “smoke test” (`--num_clients=2 --rounds=1`) that verifies correctness.  
   - Reproduces major results from the paper via the provided scripts.  

3. **Reusable**  
   - Carefully organized code (modular architecture, YAML configuration).  
   - Clear extension instructions for new datasets or neural architectures.  
   - A permissive, open-source license ensures freedom to reuse.  
   - Docker support for guaranteed consistency.


## 9. Contact and Support

- For any installation or usage issues, please open a GitHub Issue at [TraceFL-Artifact Issues](https://github.com/warisgill/TraceFL-Artifact/issues).  
- For questions related to the paper or advanced usage, contact the authors directly via their homepages.


### Citation
If you use TraceFL in your research, please cite our paper:
```bibtex
@inproceedings{gill2025tracefl,
  title = {{TraceFL: Interpretability-Driven Debugging in Federated Learning via Neuron Provenance}},
  author = {Gill, Waris and Anwar, Ali and Gulzar, Muhammad Ali},
  booktitle = {2025 IEEE/ACM 47th International Conference on Software Engineering (ICSE)},
  year = {2025},
  organization = {IEEE},
}
```

### Award Considerations

We hope that providing:
1. **A Docker image** for consistent one-click reproducibility,  
2. **Comprehensive documentation** with minimal-run examples,  
3. **Rich demonstration** of adapting to new tasks, and  
4. **Transparent licensing and archiving**,  

will make TraceFL a valuable and **exemplary** artifact for the ICSE community.

### Award Considerations
- **Cross-Domain Validation:** Works with 4 model architectures across 6 datasets
- **Scalability:** From Colab-free tier to multi-GPU clusters
- **Reproducibility:** 100% result matching via version-pinned dependencies
- **Impact:** First FL interpretability framework supporting both CV/NLP
- **Innovation:** Implements novel neuron provenance tracking methodology


**Enjoy Debugging Federated Learning with TraceFL!**  
_“Interpretability bridging the gap between global model predictions and local client contributions.”_